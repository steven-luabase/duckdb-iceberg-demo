{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import shutil\n",
    "import gcsfs\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-2' # set the region for where your s3 bucket is if different from your default region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Q2 2023 to through april 2024 (latest available data)\n",
    "trips_ls = []\n",
    "months = [\n",
    "    '2023-04',\n",
    "    '2023-05', \n",
    "    '2023-06', \n",
    "    '2023-07', \n",
    "    '2023-08', \n",
    "    '2023-09', \n",
    "    '2023-10', \n",
    "    '2023-11', \n",
    "    '2023-12', \n",
    "    '2024-01', \n",
    "    '2024-02', \n",
    "    '2024-03', \n",
    "    '2024-04'\n",
    "    ]\n",
    "for month in months:\n",
    "    table_path = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{month}.parquet'\n",
    "    # NOTE: this initial data read doesn't require Duckdb, something like pandas works as well\n",
    "    table = duckdb.sql(f\"SELECT * FROM '{table_path}'\").arrow()\n",
    "    trips_ls.append(table)\n",
    "\n",
    "# concatenate all tables\n",
    "trips = pa.concat_tables(trips_ls)\n",
    "print(\"Rows in trips: \",trips.num_rows)\n",
    "\n",
    "# get location zone mapping\n",
    "zones = duckdb.sql(\"SELECT * FROM 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\").arrow()\n",
    "print(\"Rows in zones: \",zones.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Iceberg catalog using Postgres and GCS\n",
    "catalog_name = \"demo_iceberg\"\n",
    "catalog_uri = \"<YOUR_POSTGRES_URI>\" #replace with Postgres URI\n",
    "warehouse_path = \"s3://<YOUR_S3_PATH>\" #replace with bucket name you created in S3\n",
    "\n",
    "catalog = SqlCatalog(\n",
    "    catalog_name,\n",
    "    **{\n",
    "        \"uri\": catalog_uri,\n",
    "        \"warehouse\": warehouse_path,\n",
    "    },\n",
    ")\n",
    "\n",
    "# create a namespace for Iceberg\n",
    "name_space = 'taxi'\n",
    "try:\n",
    "    catalog.create_namespace(name_space)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name='us-east-2' # replace with your s3 region\n",
    "\n",
    "def add_version_hint(iceberg_table):\n",
    "    \"\"\"\n",
    "    Adds version hint file to Iceberg table metadata\n",
    "    Addresses issue mentioned here: https://github.com/duckdb/duckdb_iceberg/issues/29\n",
    "    Determines if Iceberg table is in local file system or in GCS/S3\n",
    "    \"\"\"\n",
    "    metadata_location = iceberg_table.metadata_location\n",
    "    protocol = metadata_location.split(\":\")[0]\n",
    "\n",
    "    if protocol == \"file\":\n",
    "        metadata_location = metadata_location[7:]\n",
    "    elif protocol == \"gs\" or protocol == \"s3\":\n",
    "        metadata_location = metadata_location[5:]\n",
    "    else:\n",
    "        print(f\"Unsupported metadata location: {metadata_location}\")\n",
    "        return\n",
    "\n",
    "    metadata_dir = os.path.dirname(metadata_location)\n",
    "    new_metadata_file = os.path.join(metadata_dir, \"v1.metadata.json\")\n",
    "    version_hint_file = os.path.join(metadata_dir, \"version-hint.text\")\n",
    "\n",
    "    if protocol == \"file\":\n",
    "        shutil.copy(metadata_location, new_metadata_file)\n",
    "        with open(version_hint_file, \"w\") as f:\n",
    "            f.write(\"1\")\n",
    "    elif protocol == \"gs\":\n",
    "        fs = gcsfs.GCSFileSystem()\n",
    "        fs.copy(metadata_location, new_metadata_file)\n",
    "        with fs.open(version_hint_file, \"w\") as f:\n",
    "            f.write(\"1\")\n",
    "    elif protocol == \"s3\":\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name = metadata_location.split('/')[0]\n",
    "        s3_file_key = '/'.join(metadata_location.split('/')[1:])\n",
    "        new_s3_file_key = os.path.join(os.path.dirname(s3_file_key), \"v1.metadata.json\")\n",
    "        version_hint_key = os.path.join(os.path.dirname(s3_file_key), \"version-hint.text\")\n",
    "\n",
    "        s3.copy({'Bucket': bucket_name, 'Key': s3_file_key}, bucket_name, new_s3_file_key)\n",
    "        s3.put_object(Bucket=bucket_name, Key=version_hint_key, Body='1')\n",
    "\n",
    "    print(f\"Copied metadata file to {new_metadata_file}\")\n",
    "    print(f\"Created {version_hint_file} with content '1'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tables to iceberg catalog and load data into S3\n",
    "for table, table_name in [\n",
    "    (trips, \"trips\"),\n",
    "    (zones, \"zones\"),\n",
    "]:  \n",
    "\t# create the iceberg table\n",
    "    iceberg_table = catalog.create_table(\n",
    "        f\"{name_space}.{table_name}\",\n",
    "        schema=table.schema,\n",
    "    )\n",
    "\n",
    "    # add data to iceberg table in S3\n",
    "    iceberg_table.append(table)\n",
    "\n",
    "    # copy catalog version hint metadata into S3\n",
    "    add_version_hint(iceberg_table)\n",
    "    \n",
    "    print(f\"Created {table_name}, {table.num_rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a duckdb connection which we will use to be the query engine for iceberg\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "setup_sql = '''\n",
    "INSTALL iceberg;\n",
    "LOAD iceberg;\n",
    "\n",
    "CREATE SECRET (\n",
    "    TYPE S3,\n",
    "    KEY_ID '<YOUR_AWS_KEY>',\n",
    "    SECRET '<YOUR_AWS_SECRET_KEY>',\n",
    "    REGION '<YOUR_AWS_REGION>'\n",
    ");\n",
    "'''\n",
    "res = con.execute(setup_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the schema and views of iceberg tables in duckdb\n",
    "database_path = f'{warehouse_path}{name_space}.db'\n",
    "\n",
    "create_view_sql = f'''\n",
    "CREATE SCHEMA IF NOT EXISTS taxi;\n",
    "\n",
    "CREATE VIEW taxi.trips AS\n",
    "SELECT * FROM iceberg_scan('{database_path}/trips', allow_moved_paths = true);\n",
    "\n",
    "CREATE VIEW taxi.zones AS\n",
    "SELECT * FROM iceberg_scan('{database_path}/zones', allow_moved_paths = true);\n",
    "'''\n",
    "\n",
    "con.execute(create_view_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "select \n",
    "    count(*)\n",
    "from taxi.trips\n",
    "'''\n",
    "\n",
    "%time res = con.execute(sql)\n",
    "res.fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "select \n",
    "    date_trunc('month', tpep_pickup_datetime) as month,\n",
    "    avg(passenger_count) as avg_passenger_count,\n",
    "    avg(trip_distance) as avg_trip_distance,\n",
    "    sum(trip_distance) as total_trip_distance,\n",
    "    avg(total_amount) as avg_total_amount,\n",
    "    sum(total_amount) as total_amount,\n",
    "    count(*) as total_trips\n",
    "from taxi.trips\n",
    "-- some data pre and post our target date range is in the dataset, so we filter it out\n",
    "where tpep_pickup_datetime between '2023-04-01' and '2024-05-01'\n",
    "group by 1\n",
    "order by 1\n",
    "'''\n",
    "\n",
    "%time res = con.execute(sql)\n",
    "res.fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "select \n",
    "    zones.Borough,\n",
    "    count(*) as total_trips,\n",
    "    sum(total_amount) as total_amount\n",
    "from taxi.zones as zones\n",
    "left join taxi.trips as trips\n",
    "    on zones.LocationID = trips.DOLocationID\n",
    "group by 1 \n",
    "order by 2 desc\n",
    "'''\n",
    "\n",
    "%time res = con.execute(sql)\n",
    "res.fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "select \n",
    "    starting_zone.Borough as pickup_borough,\n",
    "    ending_zone.Borough as dropoff_borough,\n",
    "    count(*) as trip_count\n",
    "from\n",
    "taxi.trips as trips\n",
    "left join taxi.zones as starting_zone\n",
    "    on trips.PULocationID = starting_zone.LocationID\n",
    "left join taxi.zones as ending_zone\n",
    "    on trips.DOLocationID = ending_zone.LocationID\n",
    "group by 1, 2\n",
    "order by 1 asc, 3 desc\n",
    "'''\n",
    "\n",
    "%time res = con.execute(sql)\n",
    "res.fetchdf().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
